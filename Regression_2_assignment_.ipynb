{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Q1. Explain the concept of R-squared in linear regression models. How is it calculated, and what does it\n",
        "represent?"
      ],
      "metadata": {
        "id": "6sWDthP0mBIi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "answer = **R-squared in Linear Regression**\n",
        "\n",
        "R-squared, or the coefficient of determination, measures how well the regression line fits the data. It shows the proportion of variation in the dependent variable (Y) that is explained by the independent variable(s) (X).\n",
        "\n",
        "**Calculation:**\n",
        "\n",
        "   **R^2 = 1-(SSresidual/SStotal)**\n",
        "\n",
        "\n",
        "- SS_residual: Sum of squared differences between actual and predicted values.\n",
        "\n",
        "- SS_total: Sum of squared differences between actual values and their mean.\n",
        "\n",
        "**Interpretation:**\n",
        "\n",
        "- 0: Model explains none of the variance.\n",
        "- 1: Model explains all the variance.\n",
        "\n",
        "A higher R-squared means a better fit, but it doesn‚Äôt guarantee the model is good‚Äîalways check other metrics."
      ],
      "metadata": {
        "id": "rlMO_feJmh2J"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NkDB9h9-l6Pz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2. Define adjusted R-squared and explain how it differs from the regular R-squared."
      ],
      "metadata": {
        "id": "Tz-YHwrmoZkg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Answer =\n",
        "\n",
        " **Adjusted R-squared**--This measure the variation for a multiple regression model, and helps us determine goodness of fit.\n",
        "\n",
        "**Formula**\n",
        "\n",
        "Adjusted R^2 = 1-(((1-R^2)(N-1))/N-P-1)\n",
        "\n",
        "where,\n",
        "\n",
        "- N=no. of datapoints\n",
        "\n",
        "- R^2 = =R-squared\n",
        "\n",
        "- P = no. of independent feature\n",
        "\n",
        "**Key Difference:**\n",
        "\n",
        "**R-squared:** Increases or stays the same as more predictors are added, even if they don‚Äôt improve the model.\n",
        "\n",
        "**Adjusted R-squared:** Increases only if the added predictors improve the model; decreases otherwise.\n"
      ],
      "metadata": {
        "id": "b18aiueMn4H3"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0fsEJSy1oYpf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3. When is it more appropriate to use adjusted R-squared?\n",
        "\n",
        "Answer = **When to Use Adjusted R-squared**:\n",
        "\n",
        "Adjusted R-squared is more appropriate when dealing with multiple predictors in a regression model. It accounts for the number of predictors and avoids overestimating the model‚Äôs performance by penalizing irrelevant variables.\n",
        "\n",
        "**Key Scenarios:**\n",
        "- When adding predictors to check if they genuinely improve the model.\n",
        "- When comparing models with different numbers of predictors to ensure fairness."
      ],
      "metadata": {
        "id": "TEMksBH6qKVA"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FkxrS5wCqhP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4. What are RMSE, MSE, and MAE in the context of regression analysis? How are these metrics\n",
        "calculated, and what do they represent?\n",
        "\n",
        "Answer =\n",
        "\n",
        " 1. **Mean Absolute Error (MAE):**\n",
        "Measures the average absolute difference between predicted and actual values. It gives equal weight to all errors.\n",
        "\n",
        "2. **Mean Squared Error (MSE):**\n",
        "Calculates the average of squared differences between predicted and actual values. It penalizes larger errors more heavily.\n",
        "\n",
        "3. **Root Mean Squared Error (RMSE):**\n",
        "Square root of MSE. It is in the same units as the target variable, making it easier to interpret.\n",
        "\n",
        "Key Differences:\n",
        "\n",
        "- MAE: Focuses on absolute errors and is less sensitive to outliers.\n",
        "- MSE: Penalizes larger errors more due to squaring.\n",
        "- RMSE: Adds interpretability by bringing errors back to the original scale."
      ],
      "metadata": {
        "id": "3DhvOaahqrrt"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Gig4xrVBvDEh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5. Discuss the advantages and disadvantages of using RMSE, MSE, and MAE as evaluation metrics in\n",
        "regression analysis.\n",
        "\n",
        "\n",
        "Answer =\n",
        "1. **Mean Absolute Error (MAE):**\n",
        "\n",
        " - Advantages:\n",
        "Easy to understand and interpret.\n",
        "Treats all errors equally, avoiding overemphasis on outliers.\n",
        " - Disadvantages:\n",
        "May not reflect model performance well when large errors matter more.\n",
        "\n",
        "\n",
        "2. **Mean Squared Error (MSE):**\n",
        "\n",
        " - Advantages:\n",
        " 1. Penalizes large errors more, making it useful when larger deviations are critical.\n",
        " 2. Smooths gradients, aiding optimization in machine learning models.\n",
        " - Disadvantages:\n",
        " 1. Harder to interpret as it‚Äôs not in the same units as the target variable.\n",
        " 2. Overly sensitive to outliers.\n",
        "\n",
        "3. **Root Mean Squared Error (RMSE):**\n",
        "\n",
        " - Advantages:\n",
        " 1. Same units as the target variable, making interpretation intuitive.\n",
        " 2. Highlights large errors, useful when big mistakes are costly.\n",
        "- Disadvantages:\n",
        " 1. Also sensitive to outliers, similar to MSE.\n",
        " 2. More computationally intensive than MAE.\n",
        "\n",
        "\n",
        " Choosing the Metric:\n",
        "\n",
        "- Use MAE for robust evaluation with fewer outliers.\n",
        "- Use MSE or RMSE when large errors need greater emphasis."
      ],
      "metadata": {
        "id": "_7ZuZpgRvIVk"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pPTMttkewj1X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q6. Explain the concept of Lasso regularization. How does it differ from Ridge regularization, and when is\n",
        "it more appropriate to use?\n",
        "\n",
        "\n",
        "Answer= **Lasso Regularization (L1 Regularization)**: Lasso regression , or least absolute sharinkage and selection operatar is a statistical method that uses regularization to improve the accuracy of predictive models.\n",
        "\n",
        "\n",
        "**it differ from Ridge regularization**\n",
        "\n",
        " - **Lasso regression**\n",
        "Uses the L1 penalty, which penalizes the sum of the absolute values of coefficients. This allows lasso to set coefficients to zero, which is useful for feature selection.\n",
        "\n",
        "- **Ridge regression**\n",
        "Uses the L2 penalty, which penalizes the sum of the squared values of coefficients. This allows ridge regression to shrink coefficients close to zero, which is useful for reducing model complexity\n",
        "\n",
        "**When to Use Lasso:**\n",
        "\n",
        "- we need a simpler model with fewer predictors.\n",
        "- our data has high dimensionality with potentially irrelevant features.\n",
        "- Feature selection is critical for interpretability."
      ],
      "metadata": {
        "id": "LjYJSn-zwpMv"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9kh1B-uezP8_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q7. How do regularized linear models help to prevent overfitting in machine learning? Provide an\n",
        "example to illustrate.\n",
        "\n",
        "Answer =\n",
        "**Regularized Linear Models and Overfitting**\n",
        "\n",
        "Regularized models like Lasso and Ridge add a penalty to large coefficients, reducing model complexity. This prevents the model from fitting noise in the training data, which leads to overfitting.\n",
        "\n",
        "**xample:**\n",
        "In a dataset with 100 features, a plain linear regression may overfit by assigning large weights to irrelevant features. Ridge shrinks all coefficients, while Lasso eliminates irrelevant ones by setting them to zero, creating a more generalized model.\n",
        "\n",
        "**Result:**\n",
        "Regularization balances bias and variance, improving performance on unseen data."
      ],
      "metadata": {
        "id": "AJOia-3EzT4h"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NExzpDGYzr-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q8. Discuss the limitations of regularized linear models and explain why they may not always be the best\n",
        "choice for regression analysis.\n",
        "\n",
        "Answer:\n",
        "Limitations of Regularized Linear Models\n",
        "\n",
        " 1. **Interpretability Issues:**\n",
        "Regularization alters coefficients, making it harder to interpret feature importance directly.\n",
        "\n",
        "2. **Feature Scaling Required:**\n",
        "Regularized models require standardized or normalized data for proper penalty application.\n",
        "\n",
        "3. **Sensitivity to Hyperparameters:**\n",
        "Performance depends on choosing the right regularization parameter (\n",
        "ùúÜ\n",
        "Œª), requiring cross-validation.\n",
        "\n",
        "4. **Feature Selection in Ridge:**\n",
        "Ridge retains all features, even irrelevant ones, as it only shrinks coefficients, unlike Lasso.\n",
        "\n",
        "5. **Non-linear Relationships:**\n",
        "Regularized linear models assume linearity and may perform poorly with non-linear relationships without proper feature engineering.\n",
        "\n",
        "6. **Sparse Data Challenges:**\n",
        "Lasso may struggle when features are highly correlated, as it arbitrarily selects one feature and ignores others.\n",
        "\n"
      ],
      "metadata": {
        "id": "mAHPGNtIztEP"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GyFCV1q00emW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q9. You are comparing the performance of two regression models using different evaluation metrics.\n",
        "Model A has an RMSE of 10, while Model B has an MAE of 8. Which model would you choose as the better\n",
        "performer, and why? Are there any limitations to your choice of metric?\n",
        "\n",
        "Answer= Choosing Between Model A and Model B\n",
        "\n",
        "The choice depends on the context and the importance of error types:\n",
        "\n",
        "- **Model A (RMSE = 10):** RMSE penalizes larger errors more, so it‚Äôs better if large errors significantly impact outcomes.\n",
        "- **Model B (MAE = 8):** MAE treats all errors equally, making it preferable if all errors, regardless of size, are equally important.\n",
        "\n",
        "**Better Model:**\n",
        "Without more context, Model B appears better because it has a lower error value (MAE of 8).\n",
        "\n",
        "**Limitations of Metrics:**\n",
        "\n",
        "1. Different Scales: RMSE and MAE are not directly comparable as they measure errors differently.\n",
        "2. Outliers: RMSE is sensitive to outliers, which may skew results if the dataset contains extreme values.\n",
        "3. Context Dependency: The metric should align with the problem. For instance, in predicting house prices, large errors might matter more, favoring RMSE.\n",
        "\n",
        "**Conclusion:** Evaluate both metrics for each model or choose based on problem-specific priorities."
      ],
      "metadata": {
        "id": "UTIwR3J90fht"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5iRFjX911TQf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q10. You are comparing the performance of two regularized linear models using different types of\n",
        "regularization. Model A uses Ridge regularization with a regularization parameter of 0.1, while Model B\n",
        "uses Lasso regularization with a regularization parameter of 0.5. Which model would you choose as the\n",
        "better performer, and why? Are there any trade-offs or limitations to your choice of regularization\n",
        "method?\n",
        "\n",
        "Answer= Choosing Between Model A (Ridge) and Model B (Lasso)\n",
        "\n",
        "The better model depends on the dataset and goals:\n",
        "\n",
        "- Model A (Ridge): Ridge regularization (L2) is ideal when all features are relevant and multicollinearity exists, as it shrinks coefficients without setting them to zero.\n",
        "\n",
        "- Model B (Lasso): Lasso regularization (L1) is better for feature selection as it can shrink irrelevant coefficients to zero, simplifying the model.\n",
        "\n",
        "**Choice:**\n",
        "\n",
        "- If interpretability and feature selection are priorities, choose Model B.\n",
        "- If multicollinearity or the presence of many small but relevant features matters, choose Model A.\n",
        "\n",
        "**rade-offs and Limitations:**\n",
        "\n",
        "1. **Lasso:**\n",
        "\n",
        " May arbitrarily exclude one feature in correlated groups.\n",
        "Can struggle with high-dimensional data where relevant features are weakly correlated with the target.\n",
        "\n",
        "2. **Ridge:**\n",
        "\n",
        " Retains all features, even irrelevant ones, leading to less interpretability.\n",
        "May not reduce complexity as effectively as Lasso.\n",
        "\n",
        "**Conclusion:**\n",
        "The choice depends on the specific dataset, the presence of irrelevant features, and the importance of feature interpretability. Consider cross-validation to evaluate both models comprehensively.\n",
        "\n"
      ],
      "metadata": {
        "id": "pilE81Of1VgK"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N91CXHon2HAG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}